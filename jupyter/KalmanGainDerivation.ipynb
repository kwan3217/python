{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of Kalman Gain\n",
    "My first teacher for the Kalman filter referred to it as the BLUE filter:\n",
    "* **B**est - produces the smallest *a posteriori* estimate covariance\n",
    "* **L**inear - Is a linear combination of the previous estimate and the current measurement\n",
    "* **U**nbiased - The estimate is the expected value of the actual value\n",
    "* **E**stimator - Only works with measurements, with no access to the Truth\n",
    "\n",
    "The problem to find the Kalman gain is then to:\n",
    "\n",
    "* Find a form of **l**inear **e**stimator which depends on one matrix\n",
    "* Find the **b**est value for that matrix which minimizes the *a posteriori* covariance\n",
    "* Show that the estimate produced by this estimator is **u**nbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Estimator\n",
    "\n",
    "We want an estimator in general that is a linear function of the current state and the measurement:\n",
    "\n",
    "$$\n",
    "\\def\\M#1{{[\\mathbf{#1}]}}\n",
    "\\def\\MM#1#2{{[\\mathbf{#1}{#2}]}}\n",
    "\\def\\E{\\operatorname{E}}\n",
    "\\def\\cov{\\operatorname{cov}}\n",
    "\\def\\T{^\\mathsf{T}}\n",
    "$$\n",
    "\n",
    "$$\\hat{x}=\\MM{k}{^1}\\hat{x}^-+\\M{\\bar{k}}\\vec{z}$$\n",
    "\n",
    "where:\n",
    "* $\\hat{x}^-$ is the *a priori* estimate of the state, IE before taking into account the measurement. This is an $n$ element vector. It wears a hat because it is an estimate, not because it is a unit vector.\n",
    "* $\\hat{x}$ is the *a posteriori* estimate of the state, IE after taking into account the measurement. This must also be an $n$ element vector.\n",
    "* $\\vec{z}$ is the measurement. This is an $m$ element vector. It does not wear a hat because it is not an estimate, but a direct measurement.\n",
    "* $\\MM{k}{^1}$ weights the *a priori* estimate of the state. Since it transforms from state space to state space, it must be an $n \\times n$ matrix.\n",
    "* $\\M{\\bar{k}}$ transforms the measurement from measurement space to state space, and weights the measurement. Since it transforms an $m$ element vector (an $m \\times 1$ matrix) to an $n$ element vector (an $n \\times 1$ matrix), it must satisfy $n\\times 1=(n \\times m)(m \\times 1)$ and therefore be an $n \\times m$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrices $\\M{k}$ are as yet undetermined, and don't stand for *K*alman, but for *k*onstant. At this early point in the problem we have an entire universe of choices for the constants, and the choices we make determine the features of the estimator.\n",
    "\n",
    "This is guided by expectation value computations and something Grewal and Anders call the *Orthogonality principle*, but it is enough for us for now to just call it arbitrary. We can pick a $\\MM{k}{^1}$ and then solve for the $\\M{\\bar{k}}$ which satisfies certain properties and gives us the features we want. We arbitrarily choose:\n",
    "\n",
    "$$\\MM{k}{^1}=\\M{1}-\\M{\\bar{k}}\\M{H}$$\n",
    "\n",
    "where:\n",
    "* $\\M{H}$ is the matrix which transforms from state space into measurement space, IE given a state estimate $\\hat{x}$ you can calculate the ''estimated'' measurement $\\hat{z}$ as $\\hat{z}=\\M{H}\\hat{x}$. Since $\\hat{x}$ is $n \\times 1$ and $\\hat{z}$ is $m \\times 1$, for matrix multiplication compatibility we have $m\\times 1=(m\\times n)(n \\times 1)$ and therefore $\\M{H}$ is an $m \\times n$ matrix. Note that this is exactly the same definition we use on the main page for $\\M{H}$.\n",
    "\n",
    "At this point, all of the matrix sizes are determined, and we can check compatibility:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\MM{k}{^1}&=&\\M{1}&-&\\M{\\bar{k}}&\\M{H}\\\\\n",
    "n \\times n&=&(n \\times n)&-&(n \\times m)&(m \\times n)\\\\\n",
    "          &=&(n \\times n)&-&(n \\times n)\\\\\n",
    "          &=&(n \\times n)\\end{eqnarray*}$$ \n",
    "\n",
    "So, the matrix compatibility checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging this in gives us:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\hat{x}^+&=&\\left(\\M{1}-\\M{\\bar{k}}\\M{H}\\right)\\hat{x}^-+\\M{\\bar{k}}\\vec{z} \\\\\n",
    " &=&\\hat{x}^--\\M{\\bar{k}}\\M{H}\\hat{x}^-+\\M{\\bar{k}}\\vec{z} \\\\\n",
    " &=&\\hat{x}^-+\\M{\\bar{k}}\\vec{z}-\\M{\\bar{k}}\\M{H}\\hat{x}^- \\\\\n",
    " &=&\\hat{x}^-+\\M{\\bar{k}}\\left(\\vec{z}-\\M{H}\\hat{x}^-\\right)\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Completely aside from any orthogonality, we see that this gives us a form of linear estimator where the linear coefficient of the old estimate is the identity matrix, and the other coefficient multiplies the measurement residual, not just the measurement. By itself, this is pretty convenient.\n",
    "\n",
    "It turns out that part in parentheses has a simple intuitive definition: This is the *innovation*, the difference between the actual measurement $\\vec{z}$ and the measurement that we predict based on our state estimate and measurement model $\\M H \\hat{x}$. Words are important, and we don't call this the error, because we don't know whether the difference is because of an error in the measurement, an error in our estimate, or the process genuinely doing something unexpected due to process noise or unmodeled dynamics. It is sometimes called the *measurement residual*, but it more useful to save that term for the corresponding concept *after* we incorporate the measurement. We call it the innovation because it is how \"new\" information gets into our estimate. We give it the symbol $\\vec{y}=\\vec{z}-\\M H \\hat{x}^-$. Note that since the measurement $\\vec{z}$ is known (that's what it means to measure something) and the estimate is also known, we are allowed to use the innovation $\\vec{y}$ operationally.\n",
    "\n",
    "We can choose any $\\M{\\bar{k}}$ in the world, but as it turns out, there is only one choice that satisfies the *u*nbiased and *b*est criteria. Once we find that best value, we promote the $\\M{\\bar{k}}$onstant matrix to the proper $\\M{K}$alman gain matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the *a posteriori* estimate covariance matrix \n",
    "Starting with our defnintion for the *a posteriori* error covariance $\\M{P}$\n",
    "\n",
    "$$\\M{P} = \\operatorname{cov}\\left(\\vec{x} - \\hat{x}\\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\M{P}$ is the *a posteriori* estimate covariance\n",
    "* $\\vec{x}$ is the (unknown) true state. Unknown in this context just means that we are not allowed to use it in our final operational equations for any term. In this context, the true value is treated as a random variable with a known probability density function. Eventually we will restrict ourselves to a Gaussian random variable with known mean and covariance.\n",
    "* $\\hat{x}$ is the *a posteriori* estimated state. This is *not* a random variable.\n",
    "* $\\operatorname{cov}(\\vec{a})=\\operatorname E[(\\vec{a}-\\operatorname E[\\vec{a}])(\\vec{a}-\\operatorname E[\\vec{a}])\\T]$ is the covariance of any random vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we have:\n",
    "\n",
    "$$\\MM{P}{^-}=\\cov(\\vec{x}-\\hat{x}^-)$$\n",
    "\n",
    "Now we substitute in the definition of $\\hat{x}$:\n",
    "\n",
    "$$\\M{P} = \\operatorname{cov}\\left[\\vec{x} - \\left(\\hat{x}^- + \\M{\\bar{k}}\\vec{y}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and substitute $\\vec{y}=\\vec{z}-\\M H \\hat{x}$:\n",
    "\n",
    "$$\\M{P} = \\operatorname{cov}\\left(\\vec{x} - \\left[\\hat{x}^- + \\M{\\bar{k}}\\left(\\vec{z}-\\M{H}\\hat{x}^-\\right)\\right]\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\vec{z}=\\M H\\vec{x}+\\vec{v}$ where $\\vec{v}$ is the actual (unknown) noise on this measurement. We then collect on the error vectors.\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\M{P}&=&\\cov\\left(\\vec{x}-\\left[\\hat{x}^-+\\M{\\bar{k}}\\left(\\M{H}\\vec{x}+\\vec{v}-\\M{H}\\hat{x}^-\\right)\\right]\\right) \\\\\n",
    "     &=&\\cov\\left(\\vec{x}-\\left[\\hat{x}^-+\\M{\\bar{k}}\\M{H}\\vec{x}+\\M{\\bar{k}}\\vec{v}-\\M{\\bar{k}}\\M{H}\\hat{x}^-\\right]\\right) \\\\\n",
    "     &=&\\cov\\left(\\vec{x}-\\hat{x}^--\\M{\\bar{k}}\\M{H}\\vec{x}-\\M{\\bar{k}}\\vec{v}+\\M{\\bar{k}}\\M{H}\\hat{x}^-\\right) \\\\\n",
    "     &=&\\cov\\left(\\M{1}\\vec{x}-\\M{1}\\hat{x}^--\\M{\\bar{k}}\\M{H}\\vec{x}-\\M{\\bar{k}}\\vec{v}+\\M{\\bar{k}}\\M{H}\\hat{x}^-\\right) \\\\\n",
    "     &=&\\cov\\left(\\M{1}\\vec{x}-\\M{\\bar{k}}\\M{H}\\vec{x}-\\M{1}\\hat{x}^-+\\M{\\bar{k}}\\M{H}\\hat{x}^--\\M{\\bar{k}}\\vec{v}\\right) \\\\\n",
    "     &=&\\cov\\left((\\M{1}-\\M{\\bar{k}}\\M{H})\\vec{x}-\\M{1}\\hat{x}^-+\\M{\\bar{k}}\\M{H}\\hat{x}^--\\M{\\bar{k}}\\vec{v}\\right) \\\\\n",
    "     &=&\\cov\\left((\\M{1}-\\M{\\bar{k}}\\M{H})\\vec{x}-(\\M{1}\\hat{x}^--\\M{\\bar{k}}\\M{H}\\hat{x}^-)-\\M{\\bar{k}}\\vec{v}\\right) \\\\\n",
    "     &=&\\cov\\left((\\M{1}-\\M{\\bar{k}}\\M{H})\\vec{x}-(\\M{1}-\\M{\\bar{k}}\\M{H})\\hat{x}^--\\M{\\bar{k}}\\vec{v}\\right) \\\\\n",
    "     &=&\\cov\\left[\\left(\\M{1}-\\M{\\bar{k}}\\M{H}\\right)\\left(\\vec{x} - \\hat{x}^-\\right) - \\M{\\bar{k}} \\vec{v}\\right]\\end{eqnarray*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the measurement error $\\vec{v}$ is uncorrelated with the other terms, this becomes:\n",
    "\n",
    "$$\\M{P}=\\cov\\left[\\left(\\M{1}-\\M{\\bar{k}}\\M{H}\\right)\\left(\\vec{x} - \\hat{x}^-\\right)\\right] - \\cov\\left(\\M{\\bar{k}} \\vec{v}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the congruence transform this becomes:\n",
    "\n",
    "$$\\M{P} = \\left(\\M{1} - \\M{\\bar{k}} \\M{H}\\right)\\cov\\left(\\vec{x} - \\hat{x}^-\\right)\\left(\\M{1} - \\M{\\bar{k}}\\M{H}\\right)\\T + \\M{\\bar{k}}\\cov\\left(\\vec{v}\\right)\\M{K}\\T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, using the definition of $\\MM{P}{^-}$ and $\\M R$ becomes:\n",
    "\n",
    "$$\\M{P}= \\left(\\M{1}-\\M{\\bar{k}}\\M{H}\\right)\\MM{P}{^-}\\left(\\M{1}-\\M{\\bar{k}}\\M{H}\\right)\\T + \\M{\\bar{k}}\\M R\\M{\\bar{k}}\\T$$\n",
    "\n",
    "This formula (sometimes known as the *Joseph form* of the covariance update equation) is valid for any value of $\\M{\\bar{k}}$. Most interestingly, it doesn't actually depend on the measurement value -- we don't need either the truth or the measurement. All uses of the measurement have cancelled out. All places where we would want the truth, we use our estimate of the state covariance directly. We are given an initial state covariance, and we maintain the covariance with this equation which doesn't depend on the state, truth, or measurement. \n",
    "\n",
    "It turns out that if $\\M{\\bar{k}}$ is the optimal Kalman gain $\\M K$, this can be simplified further as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best estimator\n",
    "\n",
    "So, we have our arbitrary $\\M{\\bar{k}}$ matrix, for which we want to find the optimum value $\\M{K}$. Going forward, we set our two matrices $\\M{\\bar{k}}=\\M{K}$ and use whichever one seems like it best matches our intention.\n",
    "\n",
    "The error in the ''a posteriori'' state estimation is:\n",
    "$$\\vec{x} - \\hat{\\vec{x}}$$\n",
    "\n",
    "In this context, We define the *best* estimator to be the one that minimizes the expected value of the square of the magnitude of this vector: \n",
    "\n",
    "$$E\\left[\\left\\|\\vec{x} - \\hat{\\vec{x}}\\right\\|^2\\right]$$\n",
    "\n",
    "This is equivalent<sup>[[citation needed](https://xkcd.com/285/)]</sup> to minimizing the [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) of the *a posteriori* estimate [covariance matrix](https://en.wikipedia.org/wiki/covariance_matrix) $\\M P$. \n",
    "\n",
    "I don't have a proof of this, but it seems to make sense. If the $\\MM{P}{^-}$ matrix really is was accurate idea of the uncertainty before the measurement update, then the Joseph form will give us the correct $\\M{P}$ matrix after. If this matrix is \"smaller\", then it is necessary that the uncertainty in our estimate is \"smaller\", so we want the \"smallest\" $\\M{P}$ matrix consistent with the forms above. *We choose the $\\M{\\bar{k}}$ which minimizes the \"size\" of $\\M{P}$.* Since we use this same gain matrix to calculate the estimate, and since $\\M{P}$ is the smallest possible uncertainty matrix, then the estimate must be the closest possible to the actual value.\n",
    "\n",
    "in the The following is derived from the explanation on the [Wikipedia page](https://en.wikipedia.org/wiki/Kalman_filter#Derivations)\n",
    "\n",
    "By expanding out the terms in the equation above and collecting, we get:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\M{P}&=& \\left(\\M{1} - \\M{\\bar{k}}\\M{H}\\right) \\MM{P}{^-}\\left(\\M{1} - \\M{\\bar{k}}\\M{H}\\right)\\T + \\M{\\bar{k}}\\M R \\M{\\bar{k}}\\T \\\\\n",
    "     &=& \\left(\\MM{P}{^-} - \\M{\\bar{k}}\\M{H}\\MM{P}{^-}\\right)\\left(\\M{1} - \\M{\\bar{k}}\\M{H}\\right)\\T + \\M{\\bar{k}}\\M R \\M{\\bar{k}}\\T \\\\\n",
    "     &=& \\left(\\MM{P}{^-} - \\M{\\bar{k}}\\M{H}\\MM{P}{^-}\\right)\\left(\\M{1}\\T - \\left(\\M{\\bar{k}}\\M{H}\\right)\\T\\right) + \\M{\\bar{k}}\\M R \\M{\\bar{k}}\\T \\\\\n",
    "     &=& \\MM{P}{^-}\\M{1}\\T-\\MM{P}{^-}\\left(\\M{\\bar{k}}\\M{H}\\right)\\T-\\M{\\bar{k}}\\M{H}\\MM{P}{^-}\\M{1}\\T+\\M{\\bar{k}}\\M{H}\\MM{P}{^-}\\left(\\M{\\bar{k}}\\M{H}\\right)\\T + \\M{\\bar{k}}\\M R \\M{\\bar{k}}\\T \\\\\n",
    "     &=& \\MM{P}{^-}-\\MM{P}{^-}\\left(\\M{\\bar{k}}\\M{H}\\right)\\T-\\M{\\bar{k}}\\M{H}\\MM{P}{^-}+\\M{\\bar{k}}\\M{H}\\MM{P}{^-}\\left(\\M{\\bar{k}}\\M{H}\\right)\\T + \\M{\\bar{k}}\\M R \\M{\\bar{k}}\\T \\\\\n",
    "     &=& \\MM{P}{^-}-\\MM{P}{^-}\\M{H}\\T\\M{\\bar{k}}\\T-\\M{\\bar{k}}\\M{H}\\MM{P}{^-}+\\M{\\bar{k}}\\M{H}\\MM{P}{^-}\\M{H}\\T\\M{\\bar{k}}\\T + \\M{\\bar{k}}\\M R \\M{\\bar{k}}\\T \\\\\n",
    "     &=&\\MM{P}{^-}-\\M{\\bar{k}}\\M{H}\\MM{P}{^-}-\\MM{P}{^-}\\M{H}\\T\\M{\\bar{k}}\\T+\\M{\\bar{k}}\\left(\\M{H}\\MM{P}{^-}\\M{H}\\T+\\M{R}\\right)\\M{\\bar{k}}\\T \\\\\n",
    "\\M\\Gamma&=&\\M{H}\\MM{P}{^-}\\M{H}\\T+\\M{R}\\\\\n",
    "\\M P&=&\\MM{P}{^-}-\\M{\\bar{k}}\\M{H}\\MM{P}{^-}-\\MM{P}{^-}\\M{H}\\T\\M{\\bar{k}}\\T+\\M{\\bar{k}}\\M{\\Gamma}\\M{\\bar{k}}\\T\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace is minimized when its [matrix derivative](https://en.wikipedia.org/wiki/matrix_calculus) with respect to the gain matrix is zero. The derivative of a scalar function of a (not necessarily square) matrix with respect to that matrix is itself a matrix, where every element is the derivative of the scalar function with respect to the corresponding matrix element:\n",
    "\n",
    "$$\\frac{\\partial f(\\M{X})}{\\partial\\M X}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_{11}} & \\frac{\\partial f}{\\partial x_{12}} & \\cdots & \\frac{\\partial f}{\\partial x_{1q}}\\\\\n",
    "\\frac{\\partial f}{\\partial x_{21}} & \\frac{\\partial f}{\\partial x_{22}} & \\cdots & \\frac{\\partial f}{\\partial x_{2q}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f}{\\partial x_{p1}} & \\frac{\\partial f}{\\partial x_{p2}} & \\cdots & \\frac{\\partial f}{\\partial x_{pq}}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Since the trace function takes a matrix input and returns a scalar output, we see that it matches this form. The result is a matrix the same size and shape as the original $\\M X$, in particular not transposed. This is referred to as the *denominator convention* on Wikipedia.\n",
    "\n",
    "The trace of a matrix is just the sum of the elements on the main diagonal. This trace is *only* defined for square matrices, which fortunately $\\M{P}$ is. This makes most of the derivative formulas pretty simple, since the trace doesn't depend on the off-diagonal terms and therefore the derivative of the trace with respect to those terms is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are now looking for the optimum Kalman gain, we will promote $\\M{\\bar{k}}$ to $\\M K$. Using the [gradient matrix rules](https://en.wikipedia.org/wiki/matrix_calculus#Identities) (in denominator form) and the symmetry of the matrices involved we find that:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\def\\tr{\\operatorname{tr}}\n",
    "\\frac{\\partial\\tr(\\M{P})}{\\partial\\M{K}}&=&\n",
    "\\frac{\\partial\\tr\\left(\\MM{P}{^-}-\\M{K}\\M{H}\\MM{P}{^-}-\\MM{P}{^-}\\M{H}\\T\\M{K}\\T+\\M{K}\\M{\\Gamma}\\M{K}\\T\\right)}{\\partial\\M{K}} \\\\\n",
    "&=&\\frac{\\partial\\tr\\MM{P}{^-}}{\\partial\\M{K}}-\\frac{\\partial\\tr\\M{K}\\M{H}\\MM{P}{^-}}{\\partial\\M{K}}-\\frac{\\partial\\tr\\MM{P}{^-}\\M{H}\\T\\M{K}\\T}{\\partial\\M{K}}+\\frac{\\partial\\tr\\M{K}\\M{\\Gamma}\\M{K}\\T}{\\partial\\M{K}} &\\mbox{linearity of }\\tr(\\M P)\\mbox{ and differentiation}\\\\\n",
    "&=&\\M{0}-\\frac{\\partial\\tr\\M{K}\\M{H}\\MM{P}{^-}}{\\partial\\M{K}}-\\frac{\\partial\\tr\\MM{P}{^-}\\M{H}\\T\\M{K}\\T}{\\partial\\M{K}}+\\frac{\\partial\\tr\\M{K}\\M{\\Gamma}\\M{K}\\T}{\\partial\\M{K}} & \\mbox{derivative of trace that doesn't depend on }\\M K\\\\\n",
    "&=&-(\\M H\\MM{P}{^-})\\T-\\frac{\\partial\\tr\\MM{P}{^-}\\M{H}\\T\\M{K}\\T}{\\partial\\M{K}}+\\frac{\\partial\\tr\\M{K}\\M{\\Gamma}\\M{K}\\T}{\\partial\\M{K}} & \\frac{\\partial\\tr(\\M X \\M A)}{\\partial \\M X}=\\M A\\T\\\\\n",
    "&=&-(\\M H\\MM{P}{^-})\\T-\\MM{P}{^-}\\M{H}\\T+\\frac{\\partial\\tr\\M{K}\\M{\\Gamma}\\M{K}\\T}{\\partial\\M{K}} & \\frac{\\partial\\tr(\\M A \\M X\\T)}{\\partial \\M X}=\\M A\\\\\n",
    "&=&-(\\M H\\MM{P}{^-})\\T-(\\M H\\MM{P}{^-})\\T+\\frac{\\partial\\tr\\M{K}\\M{\\Gamma}\\M{K}\\T}{\\partial\\M{K}} & \\M A\\T \\M B\\T=(\\M B\\M A)\\T\\mbox{ and }\\M P\\mbox{ is symmetric}\\\\\n",
    "&=&-2\\left(\\M{H}\\MM{P}{^-}\\right)\\T+\\frac{\\partial\\tr\\M{K}\\M{\\Gamma}\\M{K}\\T}{\\partial\\M{K}}\\\\\n",
    "&=&-2\\left(\\M{H}\\MM{P}{^-}\\right)\\T+\\M{K}\\M{\\Gamma}\\T+\\M{K}\\M{\\Gamma}  & \\frac{\\partial\\tr(\\M X\\M A \\M X\\T)}{\\partial \\M X}=\\M X(\\M A+\\M A\\T)\\\\\n",
    "&=&-2\\left(\\M{H}\\MM{P}{^-}\\right)\\T+\\M{K}\\M{\\Gamma}+\\M{K}\\M{\\Gamma} & \\M \\Gamma\\mbox{ is symmetric}\\\\ \n",
    "&=&-2 \\left(\\M{H}\\MM{P}{^-}\\right)\\T + 2\\M{K}\\M{\\Gamma} \\\\\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting this to $\\M{0}$ and solving this for $\\M{K}$ yields:\n",
    "$$\\begin{eqnarray*}\n",
    "  \\M{0} &=&-2 \\left(\\M{H}\\M{P}{^-}\\right)\\T + 2\\M{K}\\M{\\Gamma} \\\\\n",
    "  2\\M{K}\\M{\\Gamma}&=&2\\left(\\M{H}\\MM{P}{^-}\\right)\\T \\\\\n",
    "  \\M{K}\\M{\\Gamma}&=&\\left(\\M{H}\\MM{P}{^-}\\right)\\T \\\\\n",
    "       &=&\\MM{P}{^-}\\M{H}\\T \\\\\n",
    "  \\M{K}&=&\\MM{P}{^-}\\M{H}\\T\\M{\\Gamma}^{-1}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "This gain, which is known as the *optimal Kalman gain*, is the one that yields [minimum mean square error](https://en.wikipedia.org/wiki/Minimum_mean_square_error) estimates when used. \n",
    "\n",
    "It's a long way from my original idea, of having a part that weights the residual and a part that transforms from measurement space to state space. We *have* done the manipulations correctly, and this *is* the gain matrix that minimizes the trace of the covariance. The gain matrix is programmed to be awesome -- by hook or by crook, this gain matrix forces the new state covariance to be the smallest, and the same gain matrix therefore gives the best estimate in the face of the given measurement. It doesn't care about transforming from measurement space to state space, but it does it anyway just because that is what is necessary to result in the smallest covariance.\n",
    "\n",
    "I don't see any explicit assumptions of Gaussian normality, on either the true state $\\vec{x}$ or measurement noise $\\vec{v}$. Maybe it is in the fact that you can only do operations on covariances when the underlying PDF really is Gaussian. Maybe it is that this definition of *optimum* only applies to Gaussian PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplification of the *a posteriori* error covariance formula\n",
    "The formula used to calculate the *a posteriori* error covariance can be simplified when the Kalman gain equals the optimal value derived above. Multiplying both sides of our Kalman gain formula on the right by $\\M{\\Gamma}\\M{K}\\T$, it follows that:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\M{K}\\M{\\Gamma}\\M{K}\\T&=&\\MM{P}{^-}\\M{H}\\T\\M{\\Gamma}^{-1}\\M{\\Gamma}\\M{K}\\T \\\\\n",
    " &=&\\MM{P}{^-}\\M{H}\\T\\M{K}\\T \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Referring back to our expanded formula for the *a posteriori* error covariance:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\M{P}&=&\\MM{P}{^-}-\\M{K}\\M{H}\\MM{P}{^-}-\\MM{P}{^-}\\M{H}\\T\\M{K}\\T+\\M{K}\\M{\\Gamma}\\M{K}\\T \\\\\n",
    "     &=&\\MM{P}{^-}-\\M{K}\\M{H}\\MM{P}{^-}-\\MM{P}{^-}\\M{H}\\T\\M{K}\\T+\\MM{P}{^-}\\M{H}\\T\\M{K}\\T \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "we find the last two terms cancel out, leaving:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\M{P}&=&\\MM{P}{^-}-\\M{K}\\M{H}\\MM{P}{^-} \\\\\n",
    "     &=&(\\M{1}-\\M{K}\\M{H})\\MM{P}{^-}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "This formula is computationally cheaper and thus nearly always used in practice, but is only correct for the optimal gain. If arithmetic precision is unusually low causing problems with [numerical stability](https://en.wikipedia.org/wiki/Numerical_stability), or if a non-optimal Kalman gain is deliberately used, this simplification cannot be applied; the *a posteriori* error covariance formula as derived above (Joseph form) must be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
