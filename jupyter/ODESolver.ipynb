{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "The prototype problem is Newton's laws and universal gravitation -- a test mass is falling freely under the influence of gravity of a central point mass. Universal Gravitation says that the force between two objects is exactly attractive (force on one is exactly towards other), proportional to the product of the masses, and inversely proportional to the square of the distance:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\vec{F}&=&\\underbrace{G}_{\\mbox{Proportionality constant}}\\overbrace{{Mm}}^{\\mbox{masses}}\\underbrace{\\frac{1}{r^2}}_{\\mbox{inverse square of distance}}\\overbrace{\\frac{-\\vec{r}}{r}}^{\\mbox{direction}}\\\\\n",
    "   &=&-\\frac{GMm\\vec{r}}{r^3}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Newton's second law says that the small mass responds to the force by accelerating in the direction of the force:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\vec{F}&=&m\\vec{a} \\\\\n",
    "\\vec{a}&=&\\frac{\\vec{F}}{m}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Newton's third law says that the force on the big mass is equal and opposite to the force on the small mass. However, we consider the big mass to be so much bigger than the small mass, that the equal force causes a negligible acceleration of the big mass. We are only going through this math so that we never have to think of it again.\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\vec{F}_{M}&=&-\\vec{F}_{m} \\\\\n",
    "M\\vec{a}_{M}&=&-m\\vec{a}_{m} \\\\\n",
    "\\vec{a}_{M}&=&-\\frac{m}{M}\\vec{a}_{m}\n",
    "\\end{eqnarray*} \\\\\n",
    "M\\gg m\\mbox{, so } \\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\frac{m}{M}&\\approx&0 \\\\\n",
    " \\vec{a}_M&=&-\\frac{m}{M}\\vec{a}_m \\\\\n",
    " &\\approx&(0)\\vec{a}_m \\\\\n",
    " &\\approx&\\vec{0}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Back to the small mass: Combining the force with the second law, we see that the mass of the small mass cancels out to get:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\vec{F}&=&m\\vec{a} \\\\\n",
    "\\vec{a}&=&\\frac{\\vec{F}}{m}\n",
    "       &=&-\\frac{GMm\\vec{r}}{r^3}\\frac{1}{m}\n",
    "       &=&-\\frac{GM\\vec{r}}{r^3}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "It is operationally much more convenient to think of the product $GM$ as a single parameter $\\mu$, since it is much easier to determine $\\mu$ from motion than either $G$ or $M$ independently. Measuring $G$ requires accurately measuring the gravity exerted by a mass that can fit in a laboratory. Measuring $M$ would require producing a large enough force on a planet in order to produce a measurable acceleration.\n",
    "\n",
    "$$\n",
    "\\vec{a}=-\\frac{\\mu\\vec{r}}{r^3}\n",
    "$$\n",
    "\n",
    "Now the definition of acceleration is the second derivative of position with respect to time. In a coordinate system where the big mass is at the origin, the vector separating the masses $\\vec{r}_{Mm}$ becomes the position vector of the small mass $\\vec{r}$. We then can put this into the language of differential equations:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\vec{a}&=&-\\frac{\\mu\\vec{r}}{r^3} \\\\\n",
    "\\frac{d^2\\vec{r}}{dt^2}&=&-\\frac{\\mu\\vec{r}}{r^3}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Now any second order equation can be cast as a system of first order equations by introducing another vector to the state -- the velocity. We end up with:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\frac{d\\vec{r}}{dt}&=&\\vec{v} \\\\\n",
    "\\frac{d\\vec{v}}{dt}&=&\\vec{a} \\\\\n",
    "                   &=&-\\frac{\\mu\\vec{r}}{r^3} \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "We could do the same thing with any higher-order equation by introducing enough extra state. Any order vector differential equation can be expressed as a system of first-order vector differential equations, and therefore we only need a first-order solver. Further, any vector first-order equation can be expressed as a system of scalar first-order equations, so we only need a scalar first-order solver.\n",
    "\n",
    "Any system which can be described by differential equations therefore has a state vector. An $n$-order differential equation of motion in $m$-dimensional space will be a system of $n \\times m$ scalar equations. The equations are allowed to depend on any combination of state vector elements, and perhaps time. In general it can be expressed as:\n",
    "\n",
    "$$\\frac{d\\vec{x}}{dt}=\\vec{f}(\\vec{x},\\vec{k},t)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\vec{x}$ is the state vector.\n",
    "* $\\vec{k}$ is a vector of parameters. In the example problem, this would be $\\mu$.\n",
    "* $t$ is *time*, the independent variable in the equations of motion. The gravity function and many other physical laws do not depend on time, but it is convenient to pass along $t$ so that the function can use it for example as a switch -- say for instance we are modeling a spacecraft, and the thruster on the spacecraft is thrusting in this direction from time $t=0$ to $t=1$, and is off at all other times. Physical laws tend to not use $t$, but control programs tend to love to use $t$.\n",
    "* $\\vec{f}()$ is the equations of motion. This function returns a vector which is the derivative of the state vector with respect to time. This means that both the numbers of components and units of each component must be correct. We can think of this equivalently as a vector function or a system of scalar functions, one for each component of the state vector. Any component of the result can depend on any combination of elements of $\\vec{x}$, $\\vec{k}$, or scalar $t$. Strictly speaking, $\\vec{k}$ could have been hard-coded into the equations, but breaking it out like this makes it cleaner to generalize into computer code.\n",
    "\n",
    "We then solve this differential equation given an initial condition. Differential equations are noted for being extremely difficult to solve symbolically -- the two-body gravity equation is right on the edge of solvability. It required the genius of Newton to solve it, and he had to invent calculus from scratch to do so. More complicated systems are generally *not* solvable in a symbolic closed-form sense.\n",
    "\n",
    "So, we take the time-honored method of *not* solving it. We do an end run around the symbolic part by doing the equation *numerically*. Remember the definition of a derivative:\n",
    "\n",
    "$$\\frac{dx}{dt}=\\lim_{dt \\to 0}\\frac{x(t+dt)-x(t)}{dt}$$\n",
    "\n",
    "We can rearrange this equation to get at the value of the function $x$ a small time into the future, like this:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\frac{dx}{dt}&=&\\lim_{dt \\to 0}\\frac{x(t+dt)-x(t)}{dt}\\\\\n",
    "\\frac{dx}{dt}&\\approx&\\frac{x(t+dt)-x(t)}{dt} \\\\\n",
    "dt\\frac{dx}{dt}&\\approx&x(t+dt)-x(t) \\\\\n",
    "x(t)+dt\\frac{dx}{dt}&\\approx&x(t+dt) \\\\\n",
    "x(t+dt)&\\approx&x(t)+dt\\frac{dx}{dt} \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "When we threw away the limit, we went from an exact equation to an approximate one. So, we hope that our equation is sufficiently well-behaved, and we use a small enough $dt$, so that the approximate value of $x(t+dt)$ is \"good enough\". There is a whole theory on exactly what \"good enough\" means, called [numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis), which we won't go into. Actually implementing this is the most obvious way of doing a numerical integrator. In fact, I wrote my first numerical integrator before I even knew what one was. This simple method has been named by history for whatever reason as the [Euler method](https://en.wikipedia.org/wiki/Euler_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler(F,dt,x0,k=None,t0=0,nstep=1):\n",
    "    \"\"\"\n",
    "    Take a fixed number of steps in a numerical integration of a differential\n",
    "    equation using the Euler method.\n",
    "    :param F: First-order vector differential equation, in the form of a Python\n",
    "       function. This function is in the form:\n",
    "       F(x,k,t)\n",
    "       where: \n",
    "         :param x: is the state or group of states vectors at the current\n",
    "                   time. The state may be a scalar, 1D numpy array, a 1xn\n",
    "                   numpy array holding a single column vector, or a 2D numpy array\n",
    "                   holding a set of vectors.\n",
    "         :param k: is the parameter vector, which may be any type, including a scalar or None.\n",
    "         :param t: is the current time, which will always be a scalar.\n",
    "       This function must produce a result with the same dimensionality as x. Note that while euler() doesn't\n",
    "       care about the form of the parameters, F() might. It is up to the user of euler() to pass in a function F() which\n",
    "       understands the parameters x and k which the user passes. The function must take all three parameters, but it \n",
    "       doesn't have to use all of them.\n",
    "    :param dt: Time step size\n",
    "    :param x0: Initial state vector. May be a scalar, 1D numpy array, 1xn numpy array holding a column vector, or a 2D\n",
    "               numpy array holding a set of vectors.\n",
    "    :param k: Optional parameter vector\n",
    "    :param t0: Optional initial time value. This is only used to pass the current time to F() in case it needs it.\n",
    "    :param nstep: Number of steps to take\n",
    "    :return: The new value of the state vector, same dimensionality as the input x0\n",
    "    \"\"\"\n",
    "    ddt=dt/nstep\n",
    "    x1=x0*1\n",
    "    for i in range(nstep):\n",
    "        x1+=ddt*F(x1,k,t0+ddt*i)\n",
    "    return x1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euler integrator, while straightforward and easy to understand, is known to be one of the weakest ones. Numerical analysis shows that it has an error size of order $O(dt)$. This means that in order to cut the error in half, you have to cut the step size in half as well, or in other words take twice as many steps to cover the same time.\n",
    "\n",
    "We will often come across problems where this error result will require taking an impractically large number of steps. More numerical analysis will allow us to construct the [fourth-order Runge-Kutta method](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#The_Runge%E2%80%93Kutta_method), an integrator with an error on the order of $O(dt^4)$. This means that halving the step size cuts the error to $1/2^4=1/16$. We do this by breaking each step into a number of substeps, figuring out the derivative at each substep, and figuring the derivative across the whole step as the weighted average of those derivatives.\n",
    "\n",
    "In particular, we do this:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "d\\vec{x}_1&=&dtF(\\vec{x}& & &,&\\vec{k},t& & &) \\\\\n",
    "d\\vec{x}_2&=&dtF(\\vec{x}&+&\\frac{d\\vec{x}_1}{2}&,&\\vec{k},t&+&\\frac{dt}{2}&) \\\\\n",
    "d\\vec{x}_3&=&dtF(\\vec{x}&+&\\frac{d\\vec{x}_2}{2}&,&\\vec{k},t&+&\\frac{dt}{2}&) \\\\\n",
    "d\\vec{x}_4&=&dtF(\\vec{x}&+&d\\vec{x}_3&,&\\vec{k},t&+&dt&)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$$\n",
    "\\vec{x}^+=\\vec{x}+\\frac{d\\vec{x}_1+2d\\vec{x}_2+2d\\vec{x}_3+d\\vec{x}_4}{6}\n",
    "$$\n",
    "\n",
    "This does:\n",
    "\n",
    "1. Figure the slope at the initial point, and from that the amount of change by multiplying the slope by the total time step.\n",
    "2. Figure the slope half way along the step, at a point determined from the initial point plus half of the change from step 1. Figure the change from this slope across the whole step.\n",
    "3. Figure the slope half way along the step, at a point determined from the initial point plus half the change from step 2. Figure the change from this slope across the whole step.\n",
    "4. Figure the slope all the way along the step, at a point determined from the initial point plus all of the change from step 3. Figure the change from across this slope across the whole step.\n",
    "\n",
    "Finally, compute the final change as the weighted average of each change, weighting changes 2 and 3 twice as heavily as 1 and 4.\n",
    "\n",
    "In terms of computational effort, to cut the error by $1/16$, we need to take two RK4 steps, each of which takes 4 evaluations of the physics function (which is assumed to dominate the computational effort), for a total of 8 evaluations. To do the same with the Euler method, we need to take 16 steps, which requires 16 evaluations. So, even though RK4 is more complicated, its error properties may make it more efficient since we can take larger (and therefore fewer) steps to cover the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4(F,dt,x0,k=None,t0=0,nstep=1):\n",
    "    \"\"\"\n",
    "    Take a fixed number of steps in a numerical integration of a differential equation using the \n",
    "    fourth-order Runge-Kutta method. This function has exactly the same interface as euler().\n",
    "    \"\"\"\n",
    "    ddt=dt/nstep\n",
    "    xp=x0*1\n",
    "    for i in range(nstep):\n",
    "        dx1=ddt*F(xp      ,k,t0+ddt*i      )\n",
    "        dx2=ddt*F(xp+dx1/2,k,t0+ddt*i+ddt/2)\n",
    "        dx3=ddt*F(xp+dx2/2,k,t0+ddt*i+ddt/2)\n",
    "        dx4=ddt*F(xp+dx3  ,k,t0+ddt*i+ddt  )\n",
    "        xp=xp+(dx1+2*dx2+2*dx3+dx4)/6\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try an example from Wikipedia:\n",
    "\n",
    "$$\\frac{dx}{dt}=x$$\n",
    "\n",
    "with an initial condition:\n",
    "\n",
    "$$\\left.x(t)\\right|_{t=0}=1$$\n",
    "\n",
    "The exact answer is: \n",
    "\n",
    "$$x=e^t$$\n",
    "\n",
    "We will compare the answers given by the Euler and Runge-Kutta methods at different step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def Fexample(x,k,t):\n",
    "    return x\n",
    "\n",
    "x0=1\n",
    "t1=4\n",
    "dt=1\n",
    "t=np.arange(0,t1+dt,dt)\n",
    "xe=np.zeros(int(t1/dt+1))\n",
    "xrk=np.zeros(int(t1/dt+1))\n",
    "xe[0]=x0\n",
    "xrk[0]=x0\n",
    "\n",
    "for i in range(int(t1/dt)):\n",
    "    xe [i+1]=euler(Fexample,dt,xe [i])\n",
    "    xrk[i+1]=rk4  (Fexample,dt,xrk[i])\n",
    "    \n",
    "plt.figure(\"dt=1\")\n",
    "plt.plot(np.arange(t1*100+1)/100,np.exp(np.arange(t1*100+1)/100),'r-',label='Exact')\n",
    "plt.plot(t,xe,'b+',label='Euler')\n",
    "plt.plot(t,xrk,'g+',label='RK4')\n",
    "plt.legend()\n",
    "plt.figure(\"error,dt=1\")\n",
    "plt.plot(t,t*0,'r-',label='Exact')\n",
    "plt.plot(t,xe-np.exp(t),'b+',label='Euler')\n",
    "plt.plot(t,xrk-np.exp(t),'g+',label='RK4')\n",
    "print(\"Euler final error: %f\"%(xe [-1]-np.exp(t[-1])))\n",
    "print(\"RK4   final error: %f\"%(xrk[-1]-np.exp(t[-1])))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both integrators are noticeably below the true value, but the Euler integrator is obviously far worse. Now we try it again, with half the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.5\n",
    "t=np.arange(0,t1+dt,dt)\n",
    "xe=np.zeros(int(t1/dt+1))\n",
    "xrk=np.zeros(int(t1/dt+1))\n",
    "xe[0]=x0\n",
    "xrk[0]=x0\n",
    "\n",
    "for i in range(int(t1/dt)):\n",
    "    xe [i+1]=euler(Fexample,dt,xe [i])\n",
    "    xrk[i+1]=rk4  (Fexample,dt,xrk[i])\n",
    "    \n",
    "plt.figure(\"dt=0.5\")\n",
    "plt.plot(np.arange(t1*100+1)/100,np.exp(np.arange(t1*100+1)/100),'r-',label='Exact')\n",
    "plt.plot(t,xe,'b+',label='Euler')\n",
    "plt.plot(t,xrk,'g+',label='RK4')\n",
    "plt.legend()\n",
    "plt.figure(\"error,dt=0.5\")\n",
    "plt.plot(t,t*0,'r-',label='Exact')\n",
    "plt.plot(t,xe-np.exp(t),'b+',label='Euler')\n",
    "plt.plot(t,xrk-np.exp(t),'g+',label='RK4')\n",
    "print(\"Euler final error: %f\"%(xe [-1]-np.exp(t[-1])))\n",
    "print(\"RK4   final error: %f\"%(xrk[-1]-np.exp(t[-1])))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't even get our expected order improvement here. We expected the error to be about half as much for Euler, and about 1/16 as much for RK4. We got an improvement of maybe 25% for Euler, and about 90% for RK4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
